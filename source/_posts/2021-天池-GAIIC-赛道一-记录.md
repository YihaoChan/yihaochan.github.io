---
title: 2021-天池-GAIIC-赛道一-记录
categories: NLP
tags: Competition
abbrlink: c516f0e3
date: 2021-06-05 22:12:16
---

# 0 前言

第一次参赛，意义特殊，记录一下。

比赛链接：https://tianchi.aliyun.com/competition/entrance/531852/introduction?lang=zh-cn

完整代码：https://github.com/YihaoChan/2021-Tianchi-GAIIC-Track1-Rank-3

<!--more-->

![](/images/2021-天池-GAIIC-赛道一-记录/证书.jpg)

![](/images/2021-天池-GAIIC-赛道一-记录/奖杯.jpg)

# 1 赛题介绍

- 提供：CT影像描述**文本**；

- 样例：右下肺野见小结节样影与软组织肿块影 => 患病区域：右下肺野，诊断类型：小结节样影与软组织肿块影；

- 需求：判断身体若干目标区域是否有异常以及异常类型.

从"若干"可判断出任务属于**多标签**任务，从"区域是否有异常"及"异常类型"可以考虑将不同区域和不同类型做**One-Hot编码**，之后进行**0/1的二分类**。简而言之，赛题属于NLP中的NLU自然语言理解领域下的多标签二分类问题。

# 2 数据样例

|    列名     |          类型          |              示例               |
| :---------: | :--------------------: | :-----------------------------: |
|  report_ID  |          int           |                1                |
| description |     文本，**脱敏**     | 101 47 12 66 74 90 0 411 234 79 |
|    label    | 区域**ID**，类型**ID** |            3 4, 0 2             |

数据没有任何明文，都是对中文进行加密后的数字，属于无明文**脱敏**数据。由此判断，无法直接调用任何已经开源的预训练权重做下游任务，如果要采用预训练模型的话，需要从头训练(**Train from scratch**)。

# 3 数据探索与分析

## 3.1 data分布

对训练集和A榜测试集的description字段数据用PCA降至二维并可视化分布图后，如图：

![](/images/2021-天池-GAIIC-赛道一-记录/distribution_train.jpg)

![](/images/2021-天池-GAIIC-赛道一-记录/distribution_test_a.jpg)

训练集和A榜测试集的data满足同分布。

## 3.2 句长分布

统计训练集和A榜测试集中不同句长所对应的description数量，如图：

![](/images/2021-天池-GAIIC-赛道一-记录/seq_len_train_set.jpg)

![](/images/2021-天池-GAIIC-赛道一-记录/seq_len_test_set_a.jpg)

训练集和A榜测试集上，最短句长均为4，最长句长分别为104和102，做句长截断、填充的时候可以参考。

## 3.3 词频统计

对训练集和A榜测试集的description字段数据进行词频统计：

训练集和A榜测试集上，编码均从0 ~ 857，词典大小均为858。词频的top2所对应的词相同，均为693、328，结合以往阅读文章的经验，可猜想这两个数字对应中文明文的逗号和句号。这也是后续我们的方案中，**去除停用词**的依据。

PS. "在构建主题模型的过程中，我们会发现'的''地''得'这样的词语无助于表达一个主题——由于这样的词语实在是太多了，在主题的词语分布中占有重要位置，导致我们总结一个主题的含义时遇到很大的困难。这个时候，去掉这些价值不大、有负作用的词语成为必需。"

"任何使用词袋模型表示文本数据的场景，都必须考虑是否需要去除停用词。这几年，我们经常使用字粒度的语言模型，比如BERT、GPT来做NLP任务。这类模型需要把句子中所有的成分都考虑进来，从而更加精准地刻画语言规律。"

# 4 模型

## 4.1 初赛

初赛我们最后融合的模型有：NEZHA、LSTM、HAN、DPCNN。

灵机一动，参考NEZHA名字的由来：

**NEZHA: *NE*ural contextuali*Z*ed representation for C*H*inese l*A*nguage understanding**

我也想了一个单词，组合我们所用的这些模型：

**Normandy: *N*EZHA, L*O*ng Sho*R*t-Term *M*emory, Hierarchical *A*ttention *N*etworks and *D*eep P*Y*ramid Convolutional Neural Networks**

主要流程：

1. 对于NEZHA预训练模型：将训练集和测试集的description进行"一句一文档"的操作进行预训练，之所以进行"一句一文档"，是为了取消BERT模型的NSP(Next Sentence Prediction)任务，因为NSP任务在BERT类模型上表现都不好，这一点已在RoBERTa论文中有提到。单进行MLM任务之后，接MLP进入微调训练阶段，结合FGM对抗训练和Warm Up + 余弦退火调整学习率；
2. 对于基础深度学习模型(LSTM、HAN、DPCNN)：先去除停用词(693、328)，然后使用Warm Up + 余弦退火进行学习率调整；
3. 两类模型均使用分层采样与k折交叉验证；
4. 最后将两部分结果进行加权融合，单模得分较高者获得更大的权重。

单模结果与融合后结果用t-SNE降至三维并可视化后，如图：

![](/images/2021-天池-GAIIC-赛道一-记录/3D_results_lstm.jpg)

![](/images/2021-天池-GAIIC-赛道一-记录/3D_results_han.jpg)

![](/images/2021-天池-GAIIC-赛道一-记录/3D_results_dpcnn.jpg)

![](/images/2021-天池-GAIIC-赛道一-记录/3D_results_nezha.jpg)

![](/images/2021-天池-GAIIC-赛道一-记录/3D_results_merge.jpg)

直观上粗略看一下，集成之后的结果"综合"了多个单模的结果，不会让部分数据太过极端，同时也有照顾到多方面的结果分布，体现出了模型融合的思想："采纳多方意见"。

## 4.2 复赛

同初赛，复赛我们所采用的模型的名字为：

**Noah's Ark: *N*EZHA Trained J*O*intly and Sep*A*rately with *H*AN, L*S*TM and Deep Pyr*A*mid Convolutional Neu*R*al Networ*K*s**

主要流程：

1. 对于NEZHA预训练模型：预训练阶段取消NSP任务，微调训练阶段采用**联合训练**(Jointly Train)与**分开训练**(Separately Train)两种策略，即：

   1. 联合训练：将两个任务拼接作为一个完整任务进行训练；
   2. 分开训练：将任务1和任务2分别用不同模型训练，之后再将结果进行拼接。恰好，初赛与复赛的任务1相同，因此可以将初赛的训练集一起加入，与复赛的训练集一起训练任务1，数据量得以提升。

   在使用相同超参数的情况下，实验结果表明，在这个数据集中，分开训练不仅能提升总得分，还能缩小两个任务之间的得分差距：

   | 策略     | 任务1得分 | 任务2得分 | 总得分 | 任务gap |
   | -------- | --------- | --------- | ------ | ------- |
   | 联合训练 | 0.9257    | 0.9142    | 0.9212 | 0.0115  |
   | 分开训练 | 0.9326    | 0.9302    | 0.9316 | 0.0024  |

   因为NEZHA单模表现实在太优秀，所以两种策略都做，之后再融合。训练阶段结合FGM对抗训练和Warm Up + 余弦退火调整学习率；

2. 对于基础深度学习模型(LSTM、HAN、DPCNN)：采用分开训练的策略，训练过程中，先去除停用词(693、328)，然后使用Warm Up + 余弦退火进行学习率调整；

3. 两类模型均使用分层采样与k折交叉验证；

4. 最后将两部分结果进行加权融合，单模得分较高者获得更大的权重。

复赛的测试集和提交结果等均在线上不可下载、不可打印，所以无可视化图。

# 5 其余尝试

1. 伪标签：第一次先用原训练集进行训练后，对测试集做推理，得出初步推理结果。之后将推理得到的output和测试集的data拼接成新的训练数据加入训练集，再对这个新的训练集进行训练，最后对测试集再做一次推理，得到提交结果。不过这个方案在我们的模型上表现不好，同时运行时间也长了很多；
2. 数据增强：随机交换句中的任意两个词、随机删除句中的部分词、将整个句子分为多段并打乱顺序。掉分原因可能是简单数据增强(EDA,  Easy Data Augmentation)所增强获得的数据质量不够高，增强后的数据分布可能破坏了原数据集的分布；
3. 对损失函数手动分配权重：由于标签类别不平衡，最多的类别所对应数据数量有1846，最少的只有209，属于类别不平衡问题。因此考虑在训练的时候给损失函数直接设定一定的比例，比例大小和标签数量成反比，使得算法能够对小类数据具有更多的"注意力"，即**对长尾数据做重加权re-weighting**。初赛时我做了这个尝试，不过效果不好。猜想：标签类别是否已经不平衡到需要手动分配权重的程度？也就是说，在这种情况下，手动分配权重是否有必要？；
4. 拼接Word2Vec和GloVe词向量做Embedding层的初始化：对于基础深度学习模型，先用Word2Vec和GloVe分别做词向量预训练，然后将这两部分词向量进行拼接，初始化Embedding层，不冻结权重，在训练过程中继续学习。可能的掉分原因：
   1. 词向量的解释性太差；
   2. 对于CBOW模型，在训练阶段中心词的词向量是把窗口大小内上下文的词向量相加作为输入，显然这忽略了文本的序列信息；
   3. 无法获得文本的情感信息，例如"好"、"坏"这两个词其使用的语境基本相同，训练出的这两个词向量会十分相似，但是其表示的情感意思却完全相反。要获得情感信息需要大量标注好的语料，需要包含不同情感得分的每个词的句子作为训练语料以获得这个词的词向量，这需要大量的相关语料；
   4. 最主要的问题是无法获得语境信息，从而无法解决一词多义的问题。因为通过Word2Vec所训练出的词向量是静态的，对于一个词无论其所处的语言环境如何最终都用相同的词向量表示。
5. 分组构造NSP任务：对于标签相同的description，将它们拼接成同一个文档，构造出一个文档中有多个句子的状况，以加入预训练过程中的NSP任务。掉分原因已在前面简述：BERT的NSP任务本身表现不是很好；
6. 加载开源权重：尝试过在预训练的时候，用华为已经训练好了的权重做初始化(除Embedding层外，其余层的信息仍然有价值)，发现与不加载权重相比，线上得分相差非常小，还不排除是云平台不同机器的原因。考虑到加载权重需要额外空间，所以最后仍然采取全代码运行、不加载权重的方案。

# 6 量化指标

1. 训练时间：Tesla V100单卡，全流程运行约42小时；
2. 推理速度：Tesla V100单卡，推理速度约320ms/条。

# 7 值得借鉴的方案

## 7.1 预训练

1. 加载开源的WWM(Whole Word Masking)全词掩码权重；
2. MLM任务使用N-gram Masking[参考ALBERT]；
3. MLM任务使用Dynamic Masking[参考RoBERTa]；
4. 将训练集的data和label拼接在一起作为新的一整条文本，即：把标签也加进去预训练，因为标签本身也自带语义；
5. 引入SBO(Span-Boundary Objective)任务[参考SpanBERT]；
6. 引入SOP(Sentence-Order Prediction)任务[参考ALBERT]；
7. 尝试通过对齐词频的方式找出分句符，制造NSP任务的语料，否则不能构造出真正的NSP任务；
8. 多任务联合训练[参考MT-DNN]。

## 7.2 微调

1. 交替训练 & 持续学习：

   1. [Don't Stop Pretraining]一文指出，要做领域自适应(DAPT, domain-adaptive pretraining)和任务自适应(TAPT, task-adaptive pretraining)的预训练；
   2. [ERNIE 2.0]提出了**持续学习**的概念，防止学习不同任务时产生灾难性遗忘。当进行多任务训练时：
      1. 先在任务1上训练；
      2. 然后使用上一步的参数对模型进行初始化，并对任务1和任务2同时训练；
      3. 再使用之前的参数对模型进行初始化，并对任务1、任务2和任务3同时训练；
      4. 以此类推。

   放在这个题目上，可以认为任务1和任务2这两个任务就满足**多任务**这一定义。因此，有两个队伍基于这个思想采用了两种不同的策略：

   1. 首先用任务1对预训练权重进行微调，之后将任务1的微调权重用于微调任务2，再反过来微调任务1，最终再微调任务2；
   2. 首先用任务2对预训练权重进行微调，之后加载上一步的权重，对任务1和任务2同时训练。

2. Lookahead；

3. SWA(Stochastic Weight Averaging，随机权重平均)。

# 8 个人心路历程

## 8.1 参赛前

二月末的时候，同班同学zyp发给了我有关这个比赛的链接，还是我们学校计算机学院公众号推送的。当时只看题目的时候，还觉得赛道一可能是数据挖掘类的题(做特征工程的那种)，赛道二明显是CV(不喜欢CV，直接跳过这个题)，赛道三是文本语义匹配，妥妥的NLP，而且肯定BERT当道的类型，当时还完全不了解BERT，只知道个名字...那个时候其实也没什么忙的事情做，而且以后也想往机器学习方面发展，总是要尝试第一次的嘛，看上去赛道一比较适合新手一点，所以就拉上之前做课程项目的固定队友lrx同学，一起决定报名赛道一了。

## 8.2 初赛

### 8.2.1 尝试

一开始一公布数据集和题目的时候，就开始自己动手实现了，仔细理解了一下题目，发现就是NLP的题。好在之前接触过一点NLP方面的东西，就对照着之前自己做的一些小demo，修修补补，抄抄删删，勉强做出来的baseline真是速度又慢、效果又差...一直很郁闷的时候，群里一位西交自动化的研究生师兄公布了他的baseline。当时跑通baseline之后，明显比我的成绩好得不要太多，就决定拿他的baseline来改了。其实后来才知道，一些热门的比赛中，会有一些选手公布自己的baseline给其他选手，算是一种分享。不过自己也不能太依赖别人的东西，是吧~

看懂baseline，然后封装、优化，形成自己的代码风格后，就开始改模型、调参了。期间多亏了zyp同学在他们实验室的服务器上(偷偷)给我开了个账号，让我连进去炼丹，具体过程见这篇博客：[记一次内网穿透](https://yihaochan.github.io/post/cb736208.html)。把baseline的CNN换上自己中意的双向LSTM之后，一度冲上30名，然后就瓶颈了...好在天池设立了一个"周周星"环节——鼓励每周的排行榜前两名分享自己的方案和心得给其他选手。不得不说这个环节真是太好了，从各位周周星那里真的学到超多比赛的方法和技巧，运用上他们所说的一部分技巧后，我的成绩又有了很大的进步。

### 8.2.2 结识新队友

其实蛮可惜的，比赛开始以来zyp同学和lrx同学都有各自的事情要忙，一位忙实验室的论文、忙着做各种实验抽不出时间，一位因为疫情原因不得不放弃出国、转投简历找工作...所以几乎相当于我一直一个人在做比赛，实在蛮累的。记得当时我线上成绩在60名左右就上不去了，刚好群里一位选手的队伍线上40名，二缺一，就缺个调BERT的，而我正好想靠BERT继续冲分(因为传统深度学习模型在NLP的比赛实在太难冲前排了...)，所以就征求了一下两位同学的意见，想过去其他的队伍，毕竟靠自己一个人真的太难太累了，也不想把成绩就此止步。两位队友表示理解之后，我就向招队友的队伍自荐了一下，最后就进入了新队伍啦。

### 8.2.3 后期

新队伍的队友是一位学姐和一位师兄。师兄发给了我BERT的keras版本baseline之后，我先花了一天了解了下BERT的原理，然后花了一下午看懂代码后优化，变成自己的风格。之后跑了个裸模型提交了一下，比我之前做的基础深度学习模型加一堆tricks和融合之后的效果好太多太多了...用的是BERT的变种——华为的NEZHA。不得不说，BERT人人都用是有道理的。切B榜之后，把NEZHA和学姐那边基础深度学习模型的结果融了一下，18名，进入复赛。

## 8.3 复赛

### 8.3.1 A榜

进入复赛后，天池要求提供打包好了的Docker镜像，用线上云平台的机器运行。这一点对我们队来说还是很友好的。我就不说了，本科没机器，硕士还没找老师干活(还在自己补基础和做比赛嘿嘿)；学姐是力学相关专业的，机器学习只是她个人兴趣，所以也没有计算资源；有机器的就师兄那边了，但是有时候一天可能要做好多实验，我这边好几个、师兄那边好几个，只用他的机器实在很麻烦。所以就学了一下Docker，试了下能不能跑通提交后，我们队就都采用线上提交的方式了，刚好测试集在线上不可下载，把代码提交到云端，还能挂载线上的测试集，把文本加进语料库做NEZHA的预训练。挺好的，还多学了Docker这项技能。不得不说一句，天池提供的镜像还蛮难用的，兼容性不是很好...

复赛的label在初赛的基础上增加多了一个任务，我就用NEZHA实现了一下，把两个任务拼接成一个总任务、做One-Hot编码，其余环节不变。当时提交的队伍还蛮少的，我们直接到了前10了，还挺惊喜的。

Well begun is half done. 看到当时的成绩，信心十足。之后进行了蛮多的尝试的：

1. 训练策略：这个是最重要的尝试：把两个任务用两个模型分别训练，最后再把推理结果拼接。刚好初赛和复赛的任务1完全一致，所以对于任务1，可以把初赛的训练集也加进来，数据量一下又增加了。一开始还蛮郁闷的，线上一直报OOM(Out of Memory)，明明在脚本入口都释放显存了呀！后来问了下另一个队伍的老哥，建议我两个模型写成两个独立的脚本，真有用，一下就搞定了，名次又往前冲到前5了好像；
2. 固定好NEZHA的折数fold和轮数epoch后，开始加tricks。对抗训练、Warm Up + 余弦退火调整学习率...把NEZHA单模调到了稳定前10，当时前10好多队伍都是用了模型融合才达到的成绩，我们单模就进了前10，所以单模的效果是真的非常好；
3. 然后把NEZHA分开训练和联合训练两个不同策略训练出来的模型的推理结果做加权平均后，又冲上去了好几名；
4. 临近复赛快结束的时候，学姐那边几个基础深度学习模型也调得差不多了，和NEZHA一融，离进决赛的资格(前6)非常非常接近。

### 8.3.2 B榜

把多个单模的运行时间分别算了一下，得出我们最后一把梭的方案全流程运行大概要42小时。而做代码管理的是我，切B榜之后提交镜像的也是我。B榜有4天的提交时间，而1天只能提交1次，相比较于其他有机器的队伍可以保存权重提交推理部分来说，他们最多可以提交4次，而我们只能提交2次，如果中间报错了，很有可能B榜都出不来成绩...提交的那天早上我心理压力实在太大了，查了很多遍路径、参数，确保万无一失才提交的镜像。提交镜像之后，实在强迫症，反反复复进入镜像又查了五六遍代码，即使查了也没用，交都交了，但是还是真的很不放心...

当然，结局是好的。第一次提交就是我们的最好成绩了，在绝大部分稳进决赛的队伍都提交了成绩之后，我们直接冲到第三。第二次提交就放飞自我了，用了更激进的参数，不过过拟合了哈哈。没事，锁定复赛第三啦！

## 8.4 决赛

被通知进入了决赛之后，赶紧开始做PPT。师兄比较有比赛经验，参考往届其他比赛的开源PPT，立马就做出一个框架出来了，之后我在框架上就添加更多细节、更多美化。其实好像初赛的时候哈哈，师兄提供baseline，我做优化。

美化好之后，PPT就发给学姐了，她是主讲人嘛，要根据她的节奏做调整。可以说，我和师兄对PPT做预训练(pretrain)，然后学姐做微调(finetune)，结合BERT哈哈。

准备期间也开了好几次会，做答辩排练，完善一下学姐的答辩过程。我们还准备了很多自问自答，想象自己如果是评委，会对我们的演讲提一些什么问题。

唯一比较遗憾的就是，我们三个人都没能在决赛现场见上面。师兄在实习不让请假，我因为看到广州有一天增加了12例直接吓怕了，不敢乱跑...所以就剩下学姐一人去杭州现场答辩啦，给我们直播会场情况。答辩过程蛮顺利的，评委问的其中一个问题还被我们押中了——"是不是融合越多模型越好？"直接搬出西瓜书的"偏差-方差窘境"，完美！另一个问题也蛮简单的，问预训练后有没有finetune，其实就是问有没有冻结权重嘛，简单。还有之前加了几位其他队伍的选手的微信，他们还夸我们的PPT做得好看~

最终的结果还是蛮安稳的，大家的排名都没有变动，第三，还是很开心滴！

## 8.5 反思

这是第一次参加大数据竞赛，真的是远超出自己的预期。最开始我起的队伍名字是：希望能完赛。从一开始抱着重在参与的心态，到后来看到自己一步步在往上爬，再与队友相识，一起冲进复赛、冲进前10、冲进决赛...感觉有些不真实，感觉自己运气很好，遇到的队友是对的，遇到的一起竞赛的选手是乐于分享的，模型选的是没错的，调的参数是准的...当然，也别太得意了，开心一下就可以了，还要学的东西实在很多很多。

想起主办方发给我的采访问题："能否讲讲指导老师给了你们哪些启发？"一开始我写下的答案只有5个字："无指导老师"。但是转头一想，每周的前两名周周星会进行方案分享，而我也会在比赛交流群里请教各位前辈，对作为新手的我来说，他们就是我的指导老师。

在知乎看到过非常扎心的一段话："很多比赛的Leaderboard里大家的分数都相差不大，造成这种现象的原因是已经有好的baseline快接近数据上限了，因此大家的分数都挤在一个小区间里。如果去找一个没有现成baseline的机器视觉比赛看看，就会发现大家的差距其实大得可怕，1000人的比赛，可能第一名accuracy到0.95，第五十名还在0.6动弹不得。"

想想我，不就是这个情况吗？一开始我自己的代码真是一团糟，得亏其他选手开源了baseline，我才能有后续的一系列成绩。还差得远呢！

继续努力。
